{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook utilised for processing results of CEC22 Manuscrip 2476\n",
    "Title: **A Transfer Learning Hyper-heuristic Approach for Automatic Tailoring of Unfolded Population-based Metaheuristics**\n",
    "Authors: Jorge M. Cruz-Duarte, Ivan Amaya, JosÃ© Carlos Ortiz-Bayliss, and Nelishia Pillay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load some packages and set some parameters for plotting and showing things\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Processing parameters\n",
    "is_saving = False  # Please, check if you want to save files\n",
    "saving_format = 'png'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "import tools as tl\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import rankdata\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import seaborn as sns\n",
    "import benchmark_func as bf\n",
    "\n",
    "# Basic collection and results for comparison:\n",
    "basic_collection_file = './collections/basicmetaheuristics.txt'\n",
    "basic_results_file = './data_files/basic-metaheuristics-data_v2.json'\n",
    "\n",
    "# Heuristic collection used in the experiments:\n",
    "collection_file = './collections/default.txt'\n",
    "\n",
    "# Datafiles from the weight matrix for comparison purposes\n",
    "datafile_names_uMHs = [\n",
    "    \"unfolded_hhs_pop30\",\n",
    "    \"unfolded_hhs_pop50\",\n",
    "    \"unfolded_hhs_pop100\"\n",
    "]\n",
    "\n",
    "# Results from the first experiment:\n",
    "datafile_names_exp1 = [\n",
    "    \"transfer_learning_dynamic_pop30_exp1\",\n",
    "    \"transfer_learning_dynamic_pop50_exp1\",\n",
    "    \"transfer_learning_dynamic_pop100_exp1\",\n",
    "    \"transfer_learning_static_pop30_exp1\",\n",
    "    \"transfer_learning_static_pop50_exp1\",\n",
    "    \"transfer_learning_static_pop100_exp1\",\n",
    "]\n",
    "\n",
    "# Output folder for the figures and results\n",
    "image_folder_name = './data_files/exp_figures/'\n",
    "datafolder_name = \"./data_files/exp_output/\"\n",
    "\n",
    "# Some lists with useful information\n",
    "chosen_categories = ['Differentiable', 'Unimodal']\n",
    "short_cat = ['Diff.', 'Unimod.']\n",
    "\n",
    "chosen_categories_inv = ['Non-differentiable', 'Multimodal']\n",
    "tac_trohs = ['Non-Diff.', 'Multimod.']\n",
    "\n",
    "case_label = 'DU'\n",
    "\n",
    "def bin2cat(bin_str):\n",
    "    \"\"\"\n",
    "    This function converts binary-based category into string-based one\n",
    "    :param bin_str: Binary string category\n",
    "    :return: Human readable string category\n",
    "    \"\"\"\n",
    "    return \" and \".join([short_cat[ichar] if (char == '1') else tac_trohs[ichar]\n",
    "                         for (ichar, char) in enumerate(str(bin_str))])\n",
    "\n",
    "# Use them to get categories\n",
    "cat_order = [bin2cat(x) for x in ['11', '10', '01', '00']]\n",
    "\n",
    "# Use if you need to disregard certain problems\n",
    "problems_to_disregard = []\n",
    "\n",
    "# Special adjustments for the plots\n",
    "sns.set(context=\"paper\", font_scale=1, palette=\"colorblind\", style=\"ticks\",\n",
    "        rc={'text.usetex': True, 'font.family': 'serif', 'font.size': 24,\n",
    "            \"xtick.major.top\": False, \"ytick.major.right\": False})\n",
    "plt.rcParams.update({'font.size': 24, 'text.usetex': True, 'font.family': 'serif'})\n",
    "\n",
    "# Read operators and find their alias\n",
    "with open(collection_file, 'r') as operators_file:\n",
    "    encoded_heuristic_space = [eval(line.rstrip('\\n')) for line in operators_file]\n",
    "\n",
    "# Search operator (perturbator and selector) aliases\n",
    "perturbator_alias = {\n",
    "    'random_search': 'RS',\n",
    "    'central_force_dynamic': 'CF',\n",
    "    'differential_mutation': 'DM',\n",
    "    'firefly_dynamic': 'FD',\n",
    "    'genetic_crossover': 'GC',\n",
    "    'genetic_mutation': 'GM',\n",
    "    'gravitational_search': 'GS',\n",
    "    'random_flight': 'RF',\n",
    "    'local_random_walk': 'RW',\n",
    "    'random_sample': 'RX',\n",
    "    'spiral_dynamic': 'SD',\n",
    "    'swarm_dynamic': 'PS'}\n",
    "\n",
    "selector_alias = {\n",
    "    'greedy': 'g',\n",
    "    'all': 'd',\n",
    "    'metropolis': 'm',\n",
    "    'probabilistic': 'p'}\n",
    "\n",
    "# Pre-process operator families\n",
    "operator_families = {y: i for i, y in enumerate(sorted([x for x in perturbator_alias.values()]))}\n",
    "\n",
    "# Pre-build the alias list\n",
    "heuristic_space = [perturbator_alias[x[0]] + selector_alias[x[2]] for x in encoded_heuristic_space]\n",
    "\n",
    "# Find repeated elements\n",
    "for heuristic in heuristic_space:\n",
    "    concurrences = tl.listfind(heuristic_space, heuristic)\n",
    "    if len(concurrences) > 1:\n",
    "        for count, idx in enumerate(concurrences):\n",
    "            heuristic_space[idx] += f'{count + 1}'\n",
    "\n",
    "# Read basic metaheuristics\n",
    "with open(basic_collection_file, 'r') as operators_file:\n",
    "    basic_mhs_collection = [eval(line.rstrip('\\n')) for line in operators_file]\n",
    "\n",
    "# Read basic metaheuristics cardinality\n",
    "basic_mhs_cadinality = [1 if isinstance(x, tuple) else len(x) for x in basic_mhs_collection]\n",
    "\n",
    "# Load data from basic metaheuristics\n",
    "basic_mhs_data = tl.read_json(basic_results_file)\n",
    "\n",
    "# Read (of create if so) a folder for storing images\n",
    "if not os.path.isdir(image_folder_name):\n",
    "    os.mkdir(image_folder_name)\n",
    "\n",
    "# Define the performance function\n",
    "def get_performance(y):\n",
    "    \"\"\"\n",
    "    Function to calculate the performance values from a fitness register\n",
    "    :param y: Array with fitness values\n",
    "    :return: Performance value = (median + interquartile_range)(fitness_values)\n",
    "    \"\"\"\n",
    "    return np.median(y) + stats.iqr(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_basic_mhs():\n",
    "    \"\"\"\n",
    "    This function only reads the dataset from basic metaheuristics and transforms it into a `pandas.DataFrame`\n",
    "    \"\"\"\n",
    "    temp_long_dimensions = basic_mhs_data['dimensions']\n",
    "\n",
    "    # Call the problem categories\n",
    "    problem_features = bf.list_functions(fts=chosen_categories)\n",
    "\n",
    "    # Create a data frame\n",
    "    data_table = pd.DataFrame({\n",
    "        'Method': ['Basic'] * len(temp_long_dimensions),\n",
    "        'Pop': [30] * len(temp_long_dimensions),\n",
    "        'Dim': [x for x in basic_mhs_data['dimensions']],\n",
    "        'Problem': basic_mhs_data['problem'],\n",
    "        'Cat': [problem_features[x]['Code'] for x in basic_mhs_data['problem']],\n",
    "        'uMH': [x['operator_id'][np.argmin(x['performance'])] for x in basic_mhs_data['results']],\n",
    "        'Performance': [get_performance(x['performance']) for x in basic_mhs_data['results']]\n",
    "    })\n",
    "\n",
    "    return data_table.sort_values(by=['Pop', 'Cat', 'Problem', 'Dim'], ignore_index=True)\n",
    "\n",
    "# Use the processing routine\n",
    "data_table_basic = process_basic_mhs()\n",
    "\n",
    "# Show the data\n",
    "data_table_basic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Define some tools to use\n",
    "### Adjusting function\n",
    "We employed the expression $\\hat{x} = \\text{sign}(x) \\log{(|x|+1)}$ to adjust the fitness values from extremely high values (in magnitude) to a short range."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the rescaling function\n",
    "def rescaling_function(values):\n",
    "    # This function rescales any fitness value to a magnitude-based representation for enhancing its comparison against other values\n",
    "    return (-1 if values < 0 else 1) * np.log10(np.abs(values) + 1)\n",
    "\n",
    "# Show how this function works\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "x_values = np.linspace(-1e3, 1e3, 5000)\n",
    "plt.plot(x_values, np.vectorize(rescaling_function)(x_values), c='r', linewidth=2)\n",
    "plt.xlabel(r\"Original value, $f$\")\n",
    "plt.ylabel(r\"Rescaled value, $\\hat{f}$\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the base dataset for comparisons\n",
    "In this part, we only read and process the base dataset for comparison purposes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the method for processing data from unfolding metaheuristics\n",
    "def process_data(dataframe_filename):\n",
    "    # Read the data file and assign the variables\n",
    "    data_frame = tl.read_json(f'data_files/{dataframe_filename}.json')\n",
    "\n",
    "    # Read all the dimensions (repeated)\n",
    "    temp_long_dimensions = data_frame['dimensions']\n",
    "\n",
    "    # Call the problem categories\n",
    "    problem_features = bf.list_functions(fts=chosen_categories)\n",
    "\n",
    "    # Create a data frame\n",
    "    data_table = pd.DataFrame({\n",
    "        'Method': ['Base'] * len(temp_long_dimensions),\n",
    "        'Pop': [int(dataframe_filename.split('pop')[-1])] * len(temp_long_dimensions),\n",
    "        'Dim': [x for x in data_frame['dimensions']],\n",
    "        'Problem': data_frame['problem'],\n",
    "        'Cat': [problem_features[x]['Code'] for x in data_frame['problem']],\n",
    "        'uMH': [x['encoded_solution'][-1] for x in data_frame['results']],\n",
    "        'Performance': [x['performance'][-1] for x in data_frame['results']],\n",
    "        'BestFitness': [[y[-1] for y in x['hist_fitness']] for x in data_frame['results']],\n",
    "        'PerformanceEvolution': [x['performance'] for x in data_frame['results']]\n",
    "    })\n",
    "\n",
    "    # Compute the metric using the rescaling function\n",
    "    data_table['Metric'] = data_table['BestFitness'].apply(\n",
    "            lambda x: [rescaling_function(y) for y in x])\n",
    "\n",
    "    return data_table\n",
    "\n",
    "# Create an empty list and process all the base dataset files\n",
    "data_table_exp0_list = list()\n",
    "for datafile_name in datafile_names_uMHs:\n",
    "    data_table_exp0_list.append(process_data(datafile_name))\n",
    "\n",
    "# Store this information in a dataframe and show it\n",
    "data_table_exp0 = pd.concat(data_table_exp0_list, ignore_index=True)\n",
    "data_table_exp0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare methods to read and process the resulting datasets\n",
    "In this case, we proceed to prepare methods to read and process the datasets from the resulting experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare the method for reading dataframes and building the datatable with pandas\n",
    "def get_datatable(dataframe_filename):\n",
    "    # Generate key for the output dictionary\n",
    "    key, pop, kind = dataframe_filename.split('_')[2:]\n",
    "\n",
    "    # Just for testing\n",
    "    full_datafile_name = datafolder_name + dataframe_filename + '.json'\n",
    "\n",
    "    data_frame = tl.read_json(full_datafile_name)\n",
    "\n",
    "    # Read dimensions and problems\n",
    "    long_dimensions = data_frame['dimensions']\n",
    "    long_problems = data_frame['problem']\n",
    "\n",
    "    dimensions = sorted(list(set(long_dimensions)))\n",
    "    problems = sorted(list(set(long_problems)))\n",
    "\n",
    "    num_prob = len(problems)\n",
    "    num_dime = len(dimensions)\n",
    "\n",
    "    # Call the problem categories\n",
    "    problem_features = bf.list_functions(fts=chosen_categories)\n",
    "\n",
    "    # Check if it is `dynamic` or `static`\n",
    "    if key == 'dynamic':\n",
    "        # Create a data frame\n",
    "        data_table = pd.DataFrame({\n",
    "            'Method': ['Dynamic'] * len(long_dimensions),\n",
    "            'Pop': [int(pop[3:])] * len(long_dimensions),\n",
    "            'Dim': [x for x in data_frame['dimensions']],\n",
    "            'Problem': data_frame['problem'],\n",
    "            'Cat': [problem_features[x]['Code'] for x in data_frame['problem']],\n",
    "            'BestFitness': [[y[-1] for y in x['details']['fitness_per_rep']] for x in data_frame['results']],\n",
    "            'Sequences': [x['details']['sequence_per_rep'] for x in data_frame['results']],\n",
    "            'FitnessEvolution': [x['details']['fitness_per_rep'] for x in data_frame['results']],\n",
    "        })\\\n",
    "            .sort_values(by=['Pop', 'Cat', 'Problem', 'Dim'], ignore_index=True)\n",
    "\n",
    "        # data_table['Metric'] = data_table['BestFitness'].apply(rescaling_function)\n",
    "        data_table['Metric'] = data_table['BestFitness'].apply(\n",
    "            lambda x: [rescaling_function(y) for y in x])\n",
    "\n",
    "        weight_table = pd.DataFrame({\n",
    "            'Pop': [int(pop[3:])] * len(long_dimensions),\n",
    "            'Dim': [str(x) for x in data_frame['dimensions']],\n",
    "            'Problem': data_frame['problem'],\n",
    "            'Cat': [problem_features[x]['Code'] for x in data_frame['problem']],\n",
    "            'Weights': [x['details']['weight_matrix'] for x in data_frame['results']],\n",
    "        })\n",
    "    else:  # 'static'\n",
    "        # Create a data frame\n",
    "        data_table = pd.DataFrame({\n",
    "            'Method': ['Static'] * len(long_dimensions),\n",
    "            'Pop': [int(pop[3:])] * len(long_dimensions),\n",
    "            'Dim': [x for x in data_frame['dimensions']],\n",
    "            'Problem': data_frame['problem'],\n",
    "            'Cat': [problem_features[x]['Code'] for x in data_frame['problem']],\n",
    "            'Performance': [x['performance'][-1] for x in data_frame['results']],\n",
    "            'BestFitness': [[y[-1] for y in x['hist_fitness']] for x in data_frame['results']],\n",
    "            'Sequences': [x['encoded_solution'][-1] for x in data_frame['results']],\n",
    "            'PerformanceEvolution': [x['performance'][::2] for x in data_frame['results']],\n",
    "        }).sort_values(by=['Pop', 'Cat', 'Problem', 'Dim'], ignore_index=True)\n",
    "\n",
    "        data_table['Metric'] = data_table['BestFitness'].apply(\n",
    "            lambda x: [rescaling_function(y) for y in x])\n",
    "\n",
    "        weight_table = None\n",
    "\n",
    "    return key + pop[3:] + kind, data_table, weight_table\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Adjusted best-fitness values\n",
    "This is a glance for the selected problems and their adjusted best-fitness values. In this case, we plot the fitness values (adjusted) using the expression $\\hat{x} = \\text{sign}(x) \\log{(|x|+1)}$, and consider all the two groups of datasets from experimental results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare methods for getting data from the datasets\n",
    "def get_experiment_data(datafile_names_exp):\n",
    "    \"\"\"\n",
    "    This method applies the above defined method to process datafiles for a given experiment dataset.\n",
    "    :param datafile_names_exp: list, list of datafile names related to experiment datasets\n",
    "    :returns: three lists: `data_info_exp_list` with the information related to experiments\n",
    "                           `data_table_exp_list` with the datasets\n",
    "                           `weight_table_exp_list` with the wight matrices\n",
    "\n",
    "    \"\"\"\n",
    "    data_info_exp_list = list()\n",
    "    data_table_exp_list = list()\n",
    "    weight_table_exp_list = list()\n",
    "\n",
    "    for datafile_name in datafile_names_exp:\n",
    "        data_info_exp, data_table_exp, weight_table_exp = get_datatable(datafile_name)\n",
    "\n",
    "        data_info_exp_list.append(data_info_exp)\n",
    "        data_table_exp_list.append(data_table_exp)\n",
    "        weight_table_exp_list.append(weight_table_exp)\n",
    "\n",
    "    return data_info_exp_list, data_table_exp_list, weight_table_exp_list\n",
    "\n",
    "\n",
    "def plot_metrics(data_info_list, data_table_list):\n",
    "    \"\"\"\n",
    "    This method is used for plotting metrics in an early state of analysis\n",
    "    :param data_info_list: list, information related to experiments\n",
    "    :param data_table_list: list, datasets\n",
    "    \"\"\"\n",
    "\n",
    "    for die2, dte2 in zip(data_info_list, data_table_list):\n",
    "        facet_out1 = sns.catplot(\n",
    "            data=dte2[['Cat', 'Problem', 'Dim', 'Metric']].explode(\n",
    "                'Metric', ignore_index=True),\n",
    "            col='Problem', y='Metric', x='Dim', col_wrap=8, kind='box', orient='v',\n",
    "            sharey=False, height=2, facet_kws=dict(despine=False, palette='dark'))\n",
    "\n",
    "        facet_out1.set_titles(col_template=\"{col_name}\")\n",
    "\n",
    "        if is_saving:\n",
    "            plt.savefig(image_folder_name + 'boxplots-{}.'.format(die2) + saving_format,\n",
    "                       format=saving_format, dpi=333, transparent=True)\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process and plot metrics for the first experiment\n",
    "In this, we use the above defined methods for showing an overview of results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_info_exp1_list, data_table_exp1_list, weight_table1_exp_list = get_experiment_data(datafile_names_exp1)\n",
    "plot_metrics(data_info_exp1_list, data_table_exp1_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data reported in the manuscript\n",
    "In this section, we organised those plot that were included in the manuscript.\n",
    "The comparison between the experiment 1 and the experiments 0 and basic metaheuristics, these two sets comprise the same problems, populations and dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge data_tables from experiment 1\n",
    "data_table_exp1 = pd.concat(data_table_exp1_list, ignore_index=True)\n",
    "data_table_exp1['Problem'].unique()\n",
    "\n",
    "# Read the problems considered for all the experiments (1 & 2)\n",
    "problems_to_consider = data_table_exp1['Problem'].unique().tolist()\n",
    "\n",
    "# Filter the data table for the experiment 0 using the problems from data_table for experiment 1\n",
    "data_table_exp0['Problem'].unique()\n",
    "\n",
    "# Filter to only use the problems\n",
    "data_table_exp0 = data_table_exp0[data_table_exp0['Problem'].isin(problems_to_consider)]\n",
    "\n",
    "# Show table from experiments 0\n",
    "data_table_exp0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Columns to consider in the concatenation\n",
    "columns_to_concatenate = ['Method', 'Pop', 'Dim', 'Problem', 'Cat', 'BestFitness']\n",
    "\n",
    "# Concatenate experiments 0 & 1\n",
    "dt_comp_exp1 = pd.concat(\n",
    "    [data_table_exp0[columns_to_concatenate]] +\n",
    "    [dt[columns_to_concatenate] for dt in data_table_exp1_list],\n",
    "    ignore_index=True)\n",
    "\n",
    "# Adjust the performance value to have the same scale\n",
    "dt_comp_exp1['Metric'] = dt_comp_exp1['BestFitness'].apply(\n",
    "    lambda x: [rescaling_function(y) for y in x])\n",
    "\n",
    "# Perform some adjustments to facilitate comparison\n",
    "dt_comp_exp1['Dim'].unique()\n",
    "dt_comp_exp1['DU'] = dt_comp_exp1['Cat'].map(bin2cat)\n",
    "dt_comp_exp1 = dt_comp_exp1.sort_values(by=['Pop', 'Method', 'Dim', 'Cat', 'Problem'], ascending=[1, 1, 1, 0, 1], ignore_index=True)\n",
    "\n",
    "# Show the resulting table\n",
    "dt_comp_exp1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Figure 2: Metric strips for all methods using DU, Pop, and Dim\n",
    "This is a compact representation for analysing metric values via grouping by categories (DU), populaiton (Pop), and dimensionalities (Dim)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Marginal plot related to Categories\n",
    "with sns.color_palette(\"colorblind\"):\n",
    "    facet_out2 = sns.catplot(\n",
    "        data=dt_comp_exp1.explode('Metric', ignore_index=True),\n",
    "        col='Pop', y='Metric', x='Method', hue='Dim', kind='strip', orient='v',\n",
    "        sharey=True, height=3, aspect=1, facet_kws=dict(despine=False, legend_out=False),\n",
    "        margin_titles=True, legend=False, dodge=True)\n",
    "    plt.subplots_adjust(hspace = 0.08, wspace=0.1)\n",
    "    facet_out2.add_legend(title='Dimensions', bbox_to_anchor=(0.98, 0.5))\n",
    "    facet_out2.set_titles(col_template=\"Population of {col_name}\")\n",
    "\n",
    "if is_saving:\n",
    "    plt.savefig(image_folder_name + 'plot1-Metric-vs-DimPop_Exp1.' + saving_format,\n",
    "               format=saving_format, dpi=333, transparent=True)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Figure 3: Metric distribution for all methods using DU, Pop, and Dim.\n",
    "This is a grid-like representation for analysing metric values via grouping by categories (DU), populaiton (Pop), and dimensionalities (Dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Marginal plot related to Population\n",
    "with sns.color_palette(\"colorblind\"):\n",
    "    facet_out2 = sns.catplot(\n",
    "        data=dt_comp_exp1.explode('Metric', ignore_index=True),\n",
    "        col='Pop', row='Dim', y='Metric', x='Method', hue='DU', kind='strip', orient='v',\n",
    "        sharey=False, height=2, aspect=1.3, margin_titles=True, dodge=True, legend=False,\n",
    "        facet_kws=dict(despine=False, legend_out=True))\n",
    "\n",
    "    facet_out2.set_titles(col_template=\"Population of {col_name}\",\n",
    "                          row_template=\"{row_name}D\")\n",
    "    plt.subplots_adjust(hspace = 0.08, wspace=0.3)\n",
    "    facet_out2.add_legend(loc='upper left', ncol=4, columnspacing=0.1,\n",
    "                          bbox_to_anchor=(0.075, 1))\n",
    "\n",
    "if is_saving:\n",
    "    plt.savefig(image_folder_name + 'plot2-Metric-vs-All_Exp1.' + saving_format,\n",
    "               format=saving_format, dpi=333, transparent=True)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Table III: Wilcoxon's test for the experiments\n",
    "In these plots, we show the $p$-value results for the Wilcoxon's test described in the manuscript"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pick data for carrying out the statistical test\n",
    "pivoted_table = dt_comp_exp1.pivot_table(\n",
    "    index=['Pop', 'Dim', 'DU', 'Problem'], columns='Method', values='perf_alt').reset_index()\n",
    "pivoted_table['BaseDynamic'] = pivoted_table['Dynamic'] - pivoted_table['Base']\n",
    "pivoted_table['BaseStatic'] = pivoted_table['Static'] - pivoted_table['Base']\n",
    "\n",
    "# Perform the test\n",
    "pValue_table1 = pivoted_table.groupby(['Pop', 'Dim'])['BaseDynamic', 'BaseStatic'].agg(\n",
    "    lambda z: stats.wilcoxon(x=z.values, alternative='less', zero_method='pratt')[-1]\n",
    ").reset_index()\n",
    "\n",
    "pValue_table1\n",
    "\n",
    "# Showing the same info but in LaTeX\n",
    "# print(\n",
    "# pValue_table1.to_latex(\n",
    "#     index=False, bold_rows=True, multirow=True, float_format=\"{:0.2e}\".format)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysing methods from a general point of view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Figure 4: Frequency of methods that rank 1st\n",
    "In this plot, we show the frequency of those methods that rank first when comparison is made using a given metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add some statistics for ranking\n",
    "new_columns = dict(\n",
    "    # max_fitness = lambda x: x['BestFitness'].apply(np.max),\n",
    "    # min_fitness = lambda x: x['BestFitness'].apply(np.min),\n",
    "    # mean_fitness = lambda x: x['BestFitness'].apply(np.mean),\n",
    "    # median_fitness = lambda x: x['BestFitness'].apply(np.median),\n",
    "    perf_alt = lambda x: x['BestFitness'].apply(lambda y: get_performance(y))\n",
    ")\n",
    "dt_comp_exp1 = dt_comp_exp1.assign(**new_columns)\n",
    "\n",
    "# Now do the ranking\n",
    "columns_to_rank = ['Pop', 'Dim', 'Problem', 'Cat']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the corresponding figure for the frequencies of first places and grouping by dimensions and methods\n",
    "def plot_comparison3_by(metric_column):\n",
    "    dt_comp_exp1['Rank'] = dt_comp_exp1.groupby(columns_to_rank)[metric_column].rank('dense')\n",
    "    dtce1_short = dt_comp_exp1.query(\"Rank == 1.0\")\n",
    "    dtce1_short = dtce1_short.groupby(['Pop', 'Dim'])['Method'].value_counts(normalize=True).rename('Frequency').reset_index()\n",
    "    with sns.color_palette(\"colorblind\"):\n",
    "        facet_out1 = sns.catplot(data=dtce1_short,\n",
    "                    hue='Method', x='Dim', y='Frequency', col='Pop', kind='bar',\n",
    "                    height=3, aspect=1, ci=True, #fill=False, alpha=0.5, levels=5, linewidths=2,  #cut=1,\n",
    "                    # warn_singular=False,\n",
    "                    facet_kws=dict(despine=False)\n",
    "                    )\n",
    "    plt.subplots_adjust(hspace = 0.08, wspace=0.1)\n",
    "    facet_out1.set_titles(col_template=\"Population of {col_name}\")\n",
    "\n",
    "    if is_saving:\n",
    "        plt.savefig(image_folder_name + 'plot4-Rank-vs-DimPop_Exp1.' + saving_format,\n",
    "                   format=saving_format, dpi=333, transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "for col in new_columns.keys():\n",
    "    print(col)\n",
    "    plot_comparison3_by(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Additional figures\n",
    "The figures and tables reported here just serve to analyse the resulting data. This information was not included due to space limitations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graphical representation of Table III: Wilcoxon's test\n",
    "In this figure, we show the graphical visualization of $p$-values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the resulting p-values\n",
    "sns.catplot(\n",
    "    data=pValue_table1.melt(\n",
    "        id_vars=['Pop', 'Dim'], value_vars=['BaseDynamic', 'BaseStatic'],\n",
    "        var_name='Test', value_name='p-Value'\n",
    "    ), x='Dim', col='Pop', y='p-Value', hue='Test', kind='bar', log=True\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extended Figure 4: Frequency of methods that rank 1st\n",
    "In this case, we considered all the grouping ways to represent these frequencies: Population, Methods, Dimensions, and Categories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_comparison11_by(metric_column):\n",
    "    dt_comp_exp1['Rank'] = dt_comp_exp1.groupby(columns_to_rank)[metric_column].rank('dense')\n",
    "    dtce1_short = dt_comp_exp1.query(\"Rank == 1.0\")\n",
    "    dtce1_short = dtce1_short.groupby(['Pop', 'Dim', 'DU'])['Method']\\\n",
    "        .value_counts(normalize=True)\\\n",
    "        .rename('Frequency')\\\n",
    "        .reset_index()\n",
    "    dtce1_short = dtce1_short.sort_values(\n",
    "        by=['Pop', 'Method', 'Dim', 'DU'],\n",
    "        ascending=[1, 1, 1, 1],\n",
    "        ignore_index=True)\n",
    "\n",
    "    with sns.color_palette(\"colorblind\"):\n",
    "        facet_out1 = sns.catplot(data=dtce1_short,\n",
    "                    hue='DU', x='Method', y='Frequency', row='Dim', col='Pop', kind='bar',\n",
    "                    height=2, aspect=1.3, ci=True, hue_order=cat_order,\n",
    "                    facet_kws=dict(despine=False, legend_out=True), margin_titles=True,\n",
    "                    legend=False)\n",
    "\n",
    "    facet_out1.set_titles(col_template=\"Population of {col_name}\",\n",
    "                          row_template=\"{row_name}D\")\n",
    "    plt.subplots_adjust(hspace = 0.08, wspace=0.1)\n",
    "    facet_out1.add_legend(loc='upper left', ncol=4, columnspacing=0.1,\n",
    "                          bbox_to_anchor=(0.075, 1))\n",
    "    if is_saving:\n",
    "        plt.savefig(image_folder_name + 'plot3-Rank-vs-All_Exp1.' + saving_format,\n",
    "                   format=saving_format, dpi=333, transparent=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Show the plot using a given metric from the `new_columns` dictionary\n",
    "for col in new_columns.keys():\n",
    "    print(col)\n",
    "    plot_comparison11_by(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison with basic metaheuristics\n",
    "These results were not included in the manuscript but show quite interesting results about the proposed method when compared against basic metaheuristics (most of them are state-of-the-art methods)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load and concatenate dataframes to perform comparisons\n",
    "\n",
    "# Columns to be concatenated\n",
    "columns_to_concatenate = ['Method', 'Pop', 'Dim', 'Problem', 'Cat', 'Performance']\n",
    "\n",
    "# We assign the comparison column\n",
    "dt_comp_exp1['Performance'] = dt_comp_exp1['perf_alt']\n",
    "\n",
    "data_table_basic = data_table_basic[data_table_basic['Problem'].isin(problems_to_consider)]\n",
    "data_table_basic = data_table_basic[data_table_basic['Dim'].isin(\n",
    "    dt_comp_exp1['Dim'].unique())]\n",
    "dt_comp_exp1_basic = pd.concat([\n",
    "    dt_comp_exp1[columns_to_concatenate], data_table_basic[columns_to_concatenate]\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only carried out the comparison with populations of 30 agents because of the available dataset of basic metaheuristics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt_comp_exp1_basic['Rank'] = dt_comp_exp1_basic.groupby(columns_to_rank)['Performance'].rank('dense')\n",
    "dt_comp_exp1_basic['DU'] = dt_comp_exp1_basic['Cat'].map(bin2cat)\n",
    "dtce1_short = dt_comp_exp1_basic.query(\"Pop == 30 & Dim == [2, 10, 30, 50] & Rank == 1.0\")\n",
    "dtce1_short = dtce1_short.groupby(['Dim', 'DU'])['Method'].value_counts(normalize=True).rename('Frequency').reset_index()\n",
    "\n",
    "with sns.color_palette(\"colorblind\"):\n",
    "    # g = sns.FacetGrid(dt_comp_exp1.query(\"Rank == 1.0\"), row='Pop', col='Cat')\n",
    "    # g.map_dataframe(lambda data, color: sns.barplot(x=data['Dim'], y=data['Method']))\n",
    "\n",
    "    facet_out1 = sns.catplot(data=dtce1_short,\n",
    "                hue='Method', x='Dim', col='DU', col_wrap=4, y='Frequency', kind='bar',\n",
    "                height=3, aspect=1, ci=True, #fill=False, alpha=0.5, levels=5, linewidths=2,  #cut=1,\n",
    "                # warn_singular=False,\n",
    "                facet_kws=dict(despine=False)\n",
    "                )\n",
    "\n",
    "    facet_out1.set_titles(col_template=\"{col_name}\")\n",
    "if is_saving:\n",
    "    plt.savefig(image_folder_name + 'plot5-rank_Pop-Dim-Method.' + saving_format,\n",
    "               format=saving_format, dpi=333, transparent=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wilcoxon's test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pivoted_table2 = dt_comp_exp1_basic.pivot_table(\n",
    "    index=['Pop', 'Dim', 'DU', 'Problem'], columns='Method', values='Performance'\n",
    ").reset_index()\n",
    "pivoted_table2['BasicBase'] = pivoted_table2['Base'] - pivoted_table2['Basic']\n",
    "pivoted_table2['BasicDynamic'] = pivoted_table2['Dynamic'] - pivoted_table2['Basic']\n",
    "pivoted_table2['BasicStatic'] = pivoted_table2['Static'] - pivoted_table2['Basic']\n",
    "pValue_table2 = pivoted_table2.groupby(['Pop', 'Dim'])[\n",
    "    'BasicBase', 'BasicDynamic', 'BasicStatic'].agg(\n",
    "    lambda z: stats.wilcoxon(x=z.values, alternative='less', zero_method='pratt')[-1]\n",
    ").reset_index()\n",
    "#\n",
    "sns.catplot(\n",
    "    data=pValue_table2.melt(\n",
    "        id_vars=['Pop', 'Dim'], value_vars=['BasicBase', 'BasicDynamic', 'BasicStatic'],\n",
    "        var_name='Test', value_name='p-Value'\n",
    "    ).query(\"Pop == 30\"), y='p-Value', x='Dim', hue='Test', kind='bar'\n",
    ")\n",
    "# pValue_table1\n",
    "pValue_table2.query(\"Pop == 30\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}